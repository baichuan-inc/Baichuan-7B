<div align="center">
      <h1> baichuan-7B </h1>
<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">
      ğŸ¤— 
      <a href="https://huggingface.co/baichuan-inc/baichuan-7B" target="_blank" style="margin-right: 15px; margin-left: 10px">Hugging Face</a> â€¢ 
        ğŸ¤–
      <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank" style="margin-left: 10px">ModelScope</a > â€¢
        <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/media/wechat.jpeg?raw=true" target="_blank" rel="noopener noreferrer" style="display: inline-block; margin-left: 10px">
      <span style="color: blue;">Wechat</span>
    </a>
    </p>


[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)
<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/README_EN.md">English</a>
    <p>
</h4>


</div>

# ä»‹ç»

baichuan-7B æ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚åŸºäº Transformer ç»“æ„ï¼Œåœ¨å¤§çº¦ 1.2 ä¸‡äº¿ tokens ä¸Šè®­ç»ƒçš„ 70 äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º 4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡æƒå¨ benchmarkï¼ˆC-EVAL/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚

# å…¬å¼€benchmarkæ¦œå•

## ä¸­æ–‡è¯„æµ‹
### C-Eval
[C-Eval æ•°æ®é›†](https://cevalbenchmark.com/index.html)æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº† 52 ä¸ªå­¦ç§‘å’Œå››ä¸ªéš¾åº¦çš„çº§åˆ«ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ•°æ®é›†çš„ dev é›†ä½œä¸º few-shot çš„æ¥æºï¼Œåœ¨ test é›†ä¸Šè¿›è¡Œäº† 5-shot æµ‹è¯•ã€‚

å…ˆä¿®æ”¹ `evaluate_zh.py` ä¸­çš„ OPENMODEL_PATH å’Œ CEVAL_DATA_PATH ä¸¤ä¸ªå€¼ï¼Œåˆ†åˆ«æ˜¯æ¨¡å‹å­˜æ”¾çš„è·¯å¾„å’Œ C-Eval æ•°æ®é›†çš„è·¯å¾„ï¼Œå†æ‰§è¡Œä¸‹é¢çš„è„šæœ¬ï¼š

```shell
shot=5  # few-shot
gpu=0  # æ˜¾å¡id
split=test  # è¯„ä¼°æµ‹è¯•é›†
model_id=baichuan-7b   # å¾…è¯„ä¼°çš„æ¨¡å‹
task=ceval  # ä»»åŠ¡åç§°ï¼šceval
echo gpu_idx-${gpu}-${model_id}_${task}_${split}_${shot}-shot
nohup python  evaluate_zh.py --gpu_idx ${gpu} --model_id ${model_id} --task ${task} --shot ${shot} --split ${split} --show_detail  > ${model_id}_${task}_${split}_${shot}-shot_record.txt 2>&1 &
```

### ç»“æœ

| Model 5-shot                | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |
|-----------------------------|---------|-----------|------|-----------------|------------|--------|
| GPT-4                       | 68.7    | 54.9      | 67.1 | 77.6            | 64.5       | 67.8   |
| ChatGPT                     | 54.4    | 41.4      | 52.9 | 61.8            | 50.9       | 53.6   |
| Claude-v1.3                 | 54.2    | 39.0      | 51.9 | 61.7            | 52.1       | 53.7   |
| Claude-instant-v1.0         | 45.9    | 35.5      | 43.1 | 53.8            | 44.2       | 45.4   |
| BLOOMZ-7B                   | 35.7    | 25.8      | 31.3 | 43.5            | 36.6       | 35.6   |
| ChatGLM-6B                  | 34.5    | 23.1      | 30.4 | 39.6            | 37.4       | 34.5   |
| Ziya-LLaMA-13B-pretrain     | 30.2    | 22.7      | 27.7 | 34.4            | 32.0       | 28.9   |
| moss-moon-003-base (16B)    | 27.4    | 24.5      | 27.0 | 29.1            | 27.2       | 26.9   |
| LLaMA-7B-hf                 | 27.1    | 25.9      | 27.1 | 26.8            | 27.9       | 26.3   |
| Falcon-7B                   | 25.8    | 24.3      | 25.8 | 26.0            | 25.8       | 25.6   |
| TigerBot-7B-base            | 25.7    | 27.0      | 27.3 | 24.7            | 23.4       | 26.1   |
| Aquila-7B<sup>*</sup>       | 25.5    | 25.2      | 25.6 | 24.6            | 25.2       | 26.6   |
| Open-LLaMA-v2-pretrain (7B) | 24.0    | 22.5      | 23.1 | 25.3            | 25.2       | 23.2   |
| BLOOM-7B                    | 22.8    | 20.2      | 21.8 | 23.3            | 23.9       | 23.3   |
| **baichuan-7B**             | 42.8    | 31.5      | 38.2 | 52.0            | 46.2       | 39.3   |


### Gaokao
[Gaokao](https://github.com/OpenLMLab/GAOKAO-Bench) æ˜¯ä¸€ä¸ªä»¥ä¸­å›½é«˜è€ƒé¢˜ä½œä¸ºè¯„æµ‹å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æ•°æ®é›†ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚
æˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œéšæœºåˆ’åˆ†åå¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œç»Ÿä¸€ 5-shot æµ‹è¯•ã€‚

### ç»“æœ
ä»¥ä¸‹æ˜¯æµ‹è¯•çš„ç»“æœã€‚

| Model            | Average |
|-------------------------|-----------------|
| BLOOMZ-7B               | 28.72           |
| LLaMA-7B                | 27.81           |
| BLOOM-7B                | 26.96           |
| TigerBot-7B-base        | 25.94           |
| Falcon-7B               | 23.98           |
| Ziya-LLaMA-13B-pretrain | 23.17           |
| ChatGLM-6B              | 21.41           |
| Open-LLaMA-v2-pretrain  | 21.41           |
| Aquila-7B<sup>*</sup>   | 24.39           |
| **baichuan-7B**         | **36.24**       |


### AGIEval
[AGIEval](https://github.com/microsoft/AGIEval) æ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„è®¤çŸ¥å’Œè§£å†³é—®é¢˜ç›¸å…³çš„ä»»åŠ¡ä¸­çš„ä¸€èˆ¬èƒ½åŠ›ã€‚
æˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å››é€‰ä¸€å•é¡¹é€‰æ‹©é¢˜ï¼Œéšæœºåˆ’åˆ†åå¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œäº†ç»Ÿä¸€ 5-shot æµ‹è¯•ã€‚

### ç»“æœ

| Model            | Average |
|-------------------------|-----------------|
| BLOOMZ-7B               | 30.27           |
| LLaMA-7B                | 28.17           |
| Ziya-LLaMA-13B-pretrain | 27.64           |
| Falcon-7B               | 27.18           |
| BLOOM-7B                | 26.55           |
| Aquila-7B<sup>*</sup>   | 25.58           |
| TigerBot-7B-base        | 25.19           |
| ChatGLM-6B              | 23.49           |
| Open-LLaMA-v2-pretrain  | 23.49           |
| **baichuan-7B**         | **34.44**       |

<sup>*</sup>å…¶ä¸­ Aquila æ¨¡å‹æ¥æºäºæ™ºæºå®˜æ–¹ç½‘ç«™(https://model.baai.ac.cn/model-detail/100098) ä»…åšå‚è€ƒ

## è‹±æ–‡æ¦œå•
é™¤äº†ä¸­æ–‡ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿæµ‹è¯•äº†æ¨¡å‹åœ¨è‹±æ–‡ä¸Šçš„æ•ˆæœï¼Œ[MMLU](https://arxiv.org/abs/2009.03300) æ˜¯åŒ…å« 57 ä¸ªå¤šé€‰ä»»åŠ¡çš„è‹±æ–‡è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†åˆç­‰æ•°å­¦ã€ç¾å›½å†å²ã€è®¡ç®—æœºç§‘å­¦ã€æ³•å¾‹ç­‰ï¼Œéš¾åº¦è¦†ç›–é«˜ä¸­æ°´å¹³åˆ°ä¸“å®¶æ°´å¹³ï¼Œæ˜¯ç›®å‰ä¸»æµçš„LLMè¯„æµ‹æ•°æ®é›†ã€‚

æˆ‘ä»¬é‡‡ç”¨äº†[å¼€æº](https://github.com/hendrycks/test) çš„è¯„æµ‹æ–¹æ¡ˆï¼Œæœ€ç»ˆ 5-shot ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

### ç»“æœ

| Model                                  | Humanities | Social Sciences | STEM | Other | Average |
|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|
| ChatGLM-6B<sup>0</sup>                 |       35.4 |      41.0       | 31.3 | 40.5  |  36.9   |
| BLOOMZ-7B<sup>0</sup>                  |       31.3 |      42.1       | 34.4 | 39.0  |  36.1   |
| mpt-7B<sup>1</sup>                     |          - |        -        |  -   |   -   |  35.6   |
| LLaMA-7B<sup>2</sup>                   |       34.0 |      38.3       | 30.5 | 38.1  |  35.1   |
| Falcon-7B<sup>1</sup>                  |          - |        -        |  -   |   -   |  35.0   |
| moss-moon-003-sft (16B)<sup>0</sup>    |       30.5 |      33.8       | 29.3 | 34.4  |  31.9   |
| BLOOM-7B<sup>0</sup>                   |       25.0 |      24.4       | 26.5 | 26.4  |  25.5   |
| moss-moon-003-base (16B)<sup>0</sup>   |       24.2 |      22.8       | 22.4 | 24.4  |  23.6   |
| **baichuan-7B<sup>0</sup>**            |   **38.4** |    **48.9**     | **35.6** | **48.1**  |  **42.3**   |

### ä¸Šæ ‡è¯´æ˜ï¼š
    0: é‡æ–°å¤ç°
    1: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
    2: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu

### å¤ç°æ–¹æ³•
```shell
git clone https://github.com/hendrycks/test
cd test
wget https://people.eecs.berkeley.edu/~hendrycks/data.tar
tar xf data
mkdir results
cp evaluate_mmlu.py .
python evaluation/evaluate_mmlu.py -m /path/to/baichuan-7b

```

å…¶ä¸­åœ¨ MMLU ä¸Š57ä¸ªä»»åŠ¡çš„å…·ä½“ç»†æŒ‡æ ‡å¦‚ä¸‹å›¾ï¼š
<p align="center">
    <br>
    <img src="media/MMLU-57-tasks.png" width="90%"/>
    <br>
</p>

å…¶ä¸­å„ä¸ªå­¦ç§‘çš„æŒ‡æ ‡å¦‚ä¸‹å›¾ï¼š
<p align="center">
    <br>
    <img src="media/MMLU 21 Subjects.png" width="90%"/>
    <br>
</p>

# æ¨ç†æ–¹æ³•

æ¨ç†ä»£ç å·²ç»åœ¨[å®˜æ–¹ Huggingface åº“](https://huggingface.co/baichuan-inc/baichuan-7B) 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B", device_map="auto", trust_remote_code=True)
inputs = tokenizer('ç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\nå¤œé›¨å¯„åŒ—->', return_tensors='pt')
inputs = inputs.to('cuda:0')
pred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))

```

# æ•°æ®

* åŸå§‹æ•°æ®åŒ…æ‹¬å¼€æºçš„ä¸­è‹±æ–‡æ•°æ®å’Œè‡ªè¡ŒæŠ“å–çš„ä¸­æ–‡äº’è”ç½‘æ•°æ®ï¼Œä»¥åŠéƒ¨åˆ†é«˜è´¨é‡çŸ¥è¯†æ€§æ•°æ®ã€‚
* å‚è€ƒç›¸å…³æ•°æ®å·¥ä½œï¼Œé¢‘ç‡å’Œè´¨é‡æ˜¯æ•°æ®å¤„ç†ç¯èŠ‚é‡ç‚¹è€ƒè™‘çš„ä¸¤ä¸ªç»´åº¦ã€‚ æˆ‘ä»¬åŸºäºå¯å‘å¼è§„åˆ™å’Œè´¨é‡æ¨¡å‹æ‰“åˆ†ï¼Œå¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œç¯‡ç« å’Œå¥å­ç²’åº¦çš„è¿‡æ»¤ã€‚åœ¨å…¨é‡æ•°æ®ä¸Šï¼Œåˆ©ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œæ–¹æ³•ï¼Œå¯¹ç¯‡ç« å’Œå¥å­ç²’åº¦åšæ»¤é‡ã€‚

æ•´ä½“æµç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š
<p align="center">
    <br>
    <img src="media/data_process.png" width="90%"/>
    <br>
</p>

* ç»è¿‡ä¸æ–­çš„è°ƒæ•´å’Œå¤šè½®æµ‹è¯•ï¼Œæœ€ç»ˆç¡®è®¤äº†ä¸€ä¸ªåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æœ€å¥½çš„ä¸­è‹±æ–‡é…æ¯”ã€‚
* æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŸºäºè‡ªåŠ¨å­¦ä¹ çš„æ•°æ®æƒé‡ç­–ç•¥ï¼Œå¯¹ä¸åŒç±»åˆ«çš„æ•°æ®è¿›è¡Œé…æ¯”ã€‚

# åˆ†è¯
æˆ‘ä»¬å‚è€ƒå­¦æœ¯ç•Œæ–¹æ¡ˆä½¿ç”¨ SentencePiece ä¸­çš„ Byte-Pair Encoding (BPE) ä½œä¸ºåˆ†è¯ç®—æ³•ï¼Œå¹¶ä¸”è¿›è¡Œäº†ä»¥ä¸‹çš„ä¼˜åŒ–ï¼š
1. ç›®å‰å¤§éƒ¨åˆ†å¼€æºæ¨¡å‹ä¸»è¦åŸºäºè‹±æ–‡ä¼˜åŒ–ï¼Œå› æ­¤å¯¹ä¸­æ–‡è¯­æ–™å­˜åœ¨æ•ˆç‡è¾ƒä½çš„é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨ 2000 ä¸‡æ¡ä»¥ä¸­è‹±ä¸ºä¸»çš„å¤šè¯­è¨€è¯­æ–™è®­ç»ƒåˆ†è¯æ¨¡å‹ï¼Œæ˜¾è‘—æå‡å¯¹äºä¸­æ–‡çš„å‹ç¼©ç‡ã€‚
2. å¯¹äºæ•°å­¦é¢†åŸŸï¼Œæˆ‘ä»¬å‚è€ƒäº† LLaMA å’Œ Galactica ä¸­çš„æ–¹æ¡ˆï¼Œå¯¹æ•°å­—çš„æ¯ä¸€ä½å•ç‹¬åˆ†å¼€ï¼Œé¿å…å‡ºç°æ•°å­—ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¹äºæå‡æ•°å­¦èƒ½åŠ›æœ‰é‡è¦å¸®åŠ©ã€‚
3. å¯¹äºç½•è§å­—è¯ï¼ˆå¦‚ç‰¹æ®Šç¬¦å·ç­‰ï¼‰ï¼Œæ”¯æŒ UTF-8 characters çš„ byte ç¼–ç ï¼Œå› æ­¤åšåˆ°æœªçŸ¥å­—è¯çš„å…¨è¦†ç›–ã€‚ 
4. æˆ‘ä»¬åˆ†æäº†ä¸åŒåˆ†è¯å™¨å¯¹è¯­æ–™çš„å‹ç¼©ç‡ï¼Œå¦‚ä¸‹è¡¨ï¼Œå¯è§æˆ‘ä»¬çš„åˆ†è¯å™¨æ˜æ˜¾ä¼˜äº LLaMA, Falcon ç­‰å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸”å¯¹æ¯”å…¶ä»–ä¸­æ–‡åˆ†è¯å™¨åœ¨å‹ç¼©ç‡ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒå’Œæ¨ç†æ•ˆç‡æ›´é«˜ã€‚

| Model         | baichuan-7B | LLaMA | Falcon | mpt-7B | ChatGLM | moss-moon-003 |
|---------------|-------------|-------|--------|--------|---------|---------------|
| Compress Rate | 0.737       | 1.312 | 1.049  | 1.206  | 0.631   | 0.659         |
| Vocab Size    | 64,000       | 32,000 | 65,024  | 50,254  | 130,344  | 106,029        |

# æ¨¡å‹ç»“æ„
æ•´ä½“æ¨¡å‹åŸºäºæ ‡å‡†çš„ Transformer ç»“æ„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å’Œ LLaMA ä¸€æ ·çš„æ¨¡å‹è®¾è®¡
* ä½ç½®ç¼–ç ï¼š[rotary-embedding](https://arxiv.org/abs/2104.09864) æ˜¯ç°é˜¶æ®µè¢«å¤§å¤šæ¨¡å‹é‡‡ç”¨çš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå…·æœ‰æ›´å¥½çš„å¤–å»¶æ•ˆæœã€‚è™½ç„¶è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§é•¿åº¦ä¸º4096ï¼Œä½†æ˜¯å®é™…æµ‹è¯•ä¸­æ¨¡å‹å¯ä»¥å¾ˆå¥½çš„æ‰©å±•åˆ° 5000 tokens ä»¥ä¸Šï¼Œå¦‚ä¸‹å›¾ï¼š
   <p align="center">
    <br>
    <img src="media/long-context-ppl.png" width="90%"/>
    <br>
     </p>
* æ¿€æ´»å±‚ï¼šSwiGLU, Feedforward å˜åŒ–ä¸º 8/3 å€çš„éšå«å±‚å¤§å°ï¼Œå³ 11,008
* Layer-Normalization: åŸºäº [RMSNorm](https://arxiv.org/abs/1910.07467) çš„ Pre-Normalization

# è®­ç»ƒç¨³å®šæ€§å’Œåå
æˆ‘ä»¬åœ¨åŸæœ¬çš„ LLaMA æ¡†æ¶ä¸Šè¿›è¡Œè¯¸å¤šä¿®æ”¹ä»¥æå‡è®­ç»ƒæ—¶çš„ååï¼Œå…·ä½“åŒ…æ‹¬ï¼š
1. ç®—å­ä¼˜åŒ–æŠ€æœ¯ï¼šé‡‡ç”¨æ›´é«˜æ•ˆç®—å­ï¼Œå¦‚ Flash-Attentionï¼ŒNVIDIA apex çš„ RMSNorm ç­‰ã€‚ 
2. ç®—å­åˆ‡åˆ†æŠ€æœ¯ï¼šå°†éƒ¨åˆ†è®¡ç®—ç®—å­è¿›è¡Œåˆ‡åˆ†ï¼Œå‡å°å†…å­˜å³°å€¼ã€‚ 
3. æ··åˆç²¾åº¦æŠ€æœ¯ï¼šé™ä½åœ¨ä¸æŸå¤±æ¨¡å‹ç²¾åº¦çš„æƒ…å†µä¸‹åŠ é€Ÿè®¡ç®—è¿‡ç¨‹ã€‚ 
4. è®­ç»ƒå®¹ç¾æŠ€æœ¯ï¼šè®­ç»ƒå¹³å°å’Œè®­ç»ƒæ¡†æ¶è”åˆä¼˜åŒ–ï¼ŒIaaS + PaaS å®ç°åˆ†é’Ÿçº§çš„æ•…éšœå®šä½å’Œä»»åŠ¡æ¢å¤ã€‚ 
5. é€šä¿¡ä¼˜åŒ–æŠ€æœ¯ï¼Œå…·ä½“åŒ…æ‹¬ï¼š 
   1. é‡‡ç”¨æ‹“æ‰‘æ„ŸçŸ¥çš„é›†åˆé€šä¿¡ç®—æ³•ï¼Œé¿å…ç½‘ç»œæ‹¥å¡é—®é¢˜ï¼Œæé«˜é€šä¿¡æ•ˆç‡ã€‚ 
   2. æ ¹æ®å¡æ•°è‡ªé€‚åº”è®¾ç½® bucket sizeï¼Œæé«˜å¸¦å®½åˆ©ç”¨ç‡ã€‚ 
   3. æ ¹æ®æ¨¡å‹å’Œé›†ç¾¤ç¯å¢ƒï¼Œè°ƒä¼˜é€šä¿¡åŸè¯­çš„è§¦å‘æ—¶æœºï¼Œä»è€Œå°†è®¡ç®—å’Œé€šä¿¡é‡å ã€‚

åŸºäºä¸Šè¿°çš„å‡ ä¸ªä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬åœ¨åƒå¡ A800 æ˜¾å¡ä¸Šè¾¾åˆ°äº† 7B æ¨¡å‹ 182 TFLOPS çš„ååï¼ŒGPU å³°å€¼ç®—åŠ›åˆ©ç”¨ç‡é«˜è¾¾ 58.3%ã€‚
   

æœ€ç»ˆçš„losså¦‚ä¸‹å›¾ï¼š
<p align="center">
    <br>
    <img src="media/7b.loss.png" width="90%"/>
    <br>
</p>


# è®­ç»ƒæ–¹æ³•
## å®‰è£…ä¾èµ–
```shell
pip install -r requirements.txt
```
## å‡†å¤‡æ•°æ®
ç”¨æˆ·å°†è®­ç»ƒè¯­æ–™æŒ‰æ€»rankæ•°çš„å€æ•°å‡åŒ€åˆ‡åˆ†æˆå¤šä¸ª UTF-8 æ–‡æœ¬æ–‡ä»¶ï¼Œæ”¾ç½®åœ¨è¯­æ–™ç›®å½•ï¼ˆé»˜è®¤ä¸º `data_dir` ï¼‰ä¸‹ã€‚å„ä¸ªrankè¿›ç¨‹å°†ä¼šè¯»å–è¯­æ–™ç›®å½•ä¸‹çš„ä¸åŒæ–‡ä»¶ï¼Œå…¨éƒ¨åŠ è½½åˆ°å†…å­˜åï¼Œå¼€å§‹åç»­è®­ç»ƒè¿‡ç¨‹ã€‚ä»¥ä¸Šæ˜¯ç®€åŒ–çš„ç¤ºèŒƒæµç¨‹ï¼Œå»ºè®®ç”¨æˆ·åœ¨æ­£å¼è®­ç»ƒä»»åŠ¡ä¸­ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´æ•°æ®ç”Ÿäº§é€»è¾‘ã€‚

## ä¸‹è½½ tokenizer æ¨¡å‹
ä¸‹è½½ tokenizer æ¨¡å‹æ–‡ä»¶ [tokenizer.model](https://huggingface.co/baichuan-inc/baichuan-7B/blob/main/tokenizer.model) ï¼Œæ”¾ç½®åœ¨é¡¹ç›®ç›®å½•ä¸‹ã€‚
   
## é…ç½® DeepSpeed
æœ¬ç¤ºèŒƒä»£ç é‡‡ç”¨ DeepSpeed æ¡†æ¶è¿›è¡Œè®­ç»ƒã€‚ç”¨æˆ·éœ€æ ¹æ®é›†ç¾¤æƒ…å†µï¼Œä¿®æ”¹ `config/hostfile` ï¼Œå¦‚æœæ˜¯å¤šæœºå¤šå¡ï¼Œéœ€è¦ä¿®æ”¹ ssh ä¸­å„ä¸ªèŠ‚ç‚¹çš„ IP é…ç½®ã€‚å…·ä½“å¯ä»¥å‚è§ DeepSpeed [å®˜æ–¹è¯´æ˜](https://www.deepspeed.ai/) ã€‚

## æ‰§è¡Œè®­ç»ƒ
```python
scripts/train.sh
```

# åè®®
å¯¹æœ¬ä»“åº“æºç çš„ä½¿ç”¨éµå¾ªå¼€æºè®¸å¯åè®® [Apache 2.0](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)ã€‚

baichuan-7B æ”¯æŒå•†ç”¨ã€‚å¦‚æœå°† baichuan-7B æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨ä½œå•†ä¸šç”¨é€”ï¼Œè¯·æ‚¨æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è”ç³»è®¸å¯æ–¹ï¼Œä»¥è¿›è¡Œç™»è®°å¹¶å‘è®¸å¯æ–¹ç”³è¯·ä¹¦é¢æˆæƒï¼šè”ç³»é‚®ç®±ï¼šopensource@baichuan-inc.comï¼Œ å…·ä½“è®¸å¯åè®®å¯è§[ã€Šbaichuan-7B æ¨¡å‹è®¸å¯åè®®ã€‹](https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)ã€‚

# Third-Party Resources

1. [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) æ”¯æŒbaichuan-7Bä½¿ç”¨Qloraè¿›è¡ŒFinetuneï¼Œæ”¯æŒRLHFï¼Œæ”¯æŒWebDemoã€‚ä½¿ç”¨ç»è¿‡sftçš„æ¨¡å‹è§ [hiyouga/baichuan-7b-sft](https://huggingface.co/hiyouga/baichuan-7b-sft)ã€‚
2. [fireballoon/baichuan-vicuna-chinese-7b](https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b) ä½¿ç”¨ ShareGPT, ShareGPT-ZH, COT & COT-ZH, Leetcode, dummyç­‰åŒ…å«ä¸­è‹±æ–‡çš„æ•°æ®Finetuneåçš„æ¨¡å‹ï¼Œè®­ç»ƒä»£ç å‚è€ƒFastChatã€‚
3. [fireballoon/baichuan-vicuna-7b](https://huggingface.co/fireballoon/baichuan-vicuna-7b) ä½¿ç”¨ShareGPT, COT å’Œ Leetcodeç­‰æ•°æ®æ··åˆFinetuneåçš„æ¨¡å‹ï¼Œè®­ç»ƒä»£ç å‚è€ƒFastChatã€‚
4. [Efficient-Tuning-LLMs](https://github.com/jianzhnie/Efficient-Tuning-LLMs) æ”¯æŒbaichuan-7Bä½¿ç”¨Qloraè¿›è¡ŒFinetuneå’Œ4bit inferenceã€‚
5. [fastllm](https://github.com/ztxz16/fastllm) fastllmæ˜¯çº¯c++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ–çš„å¤§æ¨¡å‹åº“ï¼Œæ”¯æŒbaichuan-7Båœ¨æ‰‹æœºç«¯è¿è¡Œã€‚
6. [TheBloke/baichuan-7B-GPTQ](https://huggingface.co/TheBloke/baichuan-7B-GPTQ) å¯¹baichuan-7Bçš„GPTQ 4bité‡åŒ–ã€‚

# Star History
[![Star History Chart](https://api.star-history.com/svg?repos=baichuan-inc/baichuan-7B&type=Date)](https://star-history.com/#baichuan-inc/baichuan-7B&Date)
